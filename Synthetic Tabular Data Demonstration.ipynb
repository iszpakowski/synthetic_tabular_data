{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e963810-a6b0-4b86-bafd-a9384b284a87",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* Loading & Preprocessing\n",
    "* No Synthetic Data\n",
    "    * Neural Network with No Synthetic Data\n",
    "* GAN Model\n",
    "    * Neural Network with GAN Data \n",
    "* Diffusion Model\n",
    "    * Neural Network with Diffusion Model Data\n",
    "* Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3366845-b20d-4787-8c5e-422612b920ad",
   "metadata": {},
   "source": [
    "## Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93897ba3-7d4d-4a9e-8bb8-d0bb94fefab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import scipy\n",
    "import torch\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "from numpy import zeros, ones\n",
    "from numpy.random import randn, randint\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Reshape, LeakyReLU, Dropout, GaussianNoise, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from scipy.stats import gmean\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f99f27-afd0-4f8f-9aa2-02dd6cebe9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed to 15 throughout notebook\n",
    "seed = 15\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b46a0b-5e17-44db-8eab-a601ce98bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import raw data from UCI's \"Adult\" dataset of 1994 census data\n",
    "raw_data1 = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',header=None)\n",
    "raw_data2 = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test',header=None,skiprows=1)\n",
    "raw_data = pd.concat([raw_data1,raw_data2])\n",
    "\n",
    "# add headers\n",
    "headers = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','target']\n",
    "data = raw_data.to_csv('adult_headers.csv',header=headers)\n",
    "data = pd.read_csv('adult_headers.csv')\n",
    "\n",
    "# clean data\n",
    "data = data.iloc[: , 1:]\n",
    "data['target'] = data['target'].replace([' <=50K.',' >50K.'],[' <=50K',' >50K'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732d24e-30da-4b22-be07-15793830ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical data\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "\n",
    "data = MultiColumnLabelEncoder(columns = ['workclass','education','marital-status','occupation','relationship','race','sex','native-country','target']).fit_transform(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a735b4-c0a8-434f-8e44-4b515a318174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split target variable\n",
    "X=data.iloc[: , :-1]\n",
    "y=data.iloc[: , -1:]\n",
    "\n",
    "# downsample to ~10,000 records (note that random states are set to 15 in all cases)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=15)\n",
    "X_oos=X_test\n",
    "y_oos=y_test\n",
    "X_experiment = X_train\n",
    "y_experiment = y_train\n",
    "\n",
    "# split downsampled data into test & train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_experiment, y_experiment, test_size=0.3, random_state=15)\n",
    "print ('X_train:')\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67abd45-79a5-4f42-be17-6f67b26a1f5f",
   "metadata": {},
   "source": [
    "## No Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f815a-bd95-4030-a9e9-2bcfd499e172",
   "metadata": {},
   "source": [
    "### Neural Network With No Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba6da7-14f6-42d2-8674-ff7c655b6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(14, activation='LeakyReLU', input_dim=14))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "\n",
    "# set parameters\n",
    "early_stopping_raw = EarlyStopping(monitor='val_loss', mode='min', patience=400, verbose=1)\n",
    "model_save_raw = ModelCheckpoint('nn_raw_best_model.h5', save_best_only=True, monitor='val_auc', mode='max', verbose=1)\n",
    "\n",
    "#train model             \n",
    "nn_model_no_synthetic = nn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2000, batch_size=32, verbose=1, callbacks=[early_stopping_raw, model_save_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac6dc7-2c30-4e98-a6c8-5e5e4588e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "pyplot.plot(nn_model_no_synthetic.history['auc'], label='train')\n",
    "pyplot.plot(nn_model_no_synthetic.history['val_auc'], label='test')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0083acb-a569-4a92-a720-4f19a93f362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "nn_raw_model = load_model('nn_raw_best_model.h5')\n",
    "\n",
    "# evaluate model on test data (how model would be evaluated at the time)\n",
    "predictions = nn_raw_model.predict(X_test)\n",
    "nn_raw_model_auc_test = roc_auc_score(y_test, predictions)\n",
    "print(\"Neural Net Raw Model AUC on Test Data: \" + str(round(nn_raw_model_auc_test,4)))\n",
    "\n",
    "# evaluate model on out of sample data (simulating a production environment)\n",
    "predictions = nn_raw_model.predict(X_oos)\n",
    "nn_raw_model_auc_oos = roc_auc_score(y_oos, predictions)\n",
    "print(\"Neural Net Raw Model AUC on Out of Sample Data: \" + str(round(nn_raw_model_auc_oos,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37145c-c917-4a50-8f0a-ae0739e12deb",
   "metadata": {},
   "source": [
    "## GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44672cd-9902-481e-806f-23e88a02c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize training data between -1 and 1\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_adj = scaler.fit_transform(X_train)  \n",
    "\n",
    "# define generator\n",
    "def build_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    model.add(GaussianNoise(0.1, seed=15, input_shape=(latent_dim,)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(latent_dim, activation='tanh'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['AUC'])\n",
    "    return model\n",
    "    \n",
    "# define discriminator\n",
    "def build_discriminator(latent_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(14, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "# define GAN\n",
    "def define_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "# generate real data points\n",
    "def generate_real_samples(X_train_adj, half_batch):\n",
    "    ix = randint(0, X_train_adj.shape[0], half_batch) \n",
    "    X = X_train_adj[ix]  \n",
    "    y = ones((half_batch, 1)) \n",
    "    return X, y\n",
    "\n",
    "# generate latent data points\n",
    "def generate_latent_points(latent_dim, n_batch):\n",
    "    x_input = randn(latent_dim * n_batch)  \n",
    "    z_input = x_input.reshape(n_batch, latent_dim)\n",
    "    return z_input\n",
    "\n",
    "# generate fake data points\n",
    "def generate_fake_samples(generator, latent_dim, half_batch):\n",
    "    z_input = generate_latent_points(latent_dim, half_batch)\n",
    "    x_synthetic = generator.predict(z_input)  \n",
    "    y = zeros((half_batch, 1))\n",
    "    return x_synthetic, y\n",
    "\n",
    "# save model \n",
    "def save_model(generator, discriminator, n_epochs):\n",
    "    generator_filename = 'generator_model_%03d.h5' % (n_epochs+1)\n",
    "    generator.save(generator_filename)\n",
    "    discriminator_filename = 'discriminator_model_%03d.h5' % (n_epochs+1)\n",
    "    discriminator.save(discriminator_filename)\n",
    "\n",
    "# plot results\n",
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "    pyplot.plot(d1_hist, label='d-real')\n",
    "    pyplot.plot(d2_hist, label='d-fake')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig('gan_history_plot.png')\n",
    "    pyplot.close()    \n",
    "\n",
    "# define model training\n",
    "def train(generator, discriminator, gan_model, X_train_adj, latent_dim, n_epochs, n_batch, half_batch):\n",
    "    batch_per_epoch = int(X_train_adj.shape[0] / n_batch)\n",
    "    n_steps = batch_per_epoch * n_epochs\n",
    "    d1_hist, d2_hist, g_hist = list(), list(), list()\n",
    "    for t in range(n_epochs):\n",
    "      for i in range(batch_per_epoch):\n",
    "        X_real, y_real = generate_real_samples(X_train_adj, half_batch)\n",
    "        d_loss_r, auc_r = discriminator.train_on_batch(X_real, y_real)\n",
    "        X_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
    "        d_loss_f, auc_f = discriminator.train_on_batch(X_fake, y_fake)\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch) \n",
    "        y_gan = ones((n_batch, 1)) \n",
    "        g_loss, g_auc = gan_model.train_on_batch(x_gan, y_gan)\n",
    "        d1_hist.append(d_loss_r)\n",
    "        d2_hist.append(d_loss_f)\n",
    "        g_hist.append(g_loss)\n",
    "      if (t+1) % 5 == 0:\n",
    "        save_model(generator, discriminator, t)\n",
    "        print(f\"epoch {t+1}/{n_epochs}, D loss on real {round(d_loss_r,4)}, D loss on fake {round(d_loss_f,4)}, G loss {round(g_loss,4)}\") \n",
    "    plot_history(d1_hist, d2_hist, g_hist)    \n",
    "    \n",
    "# set hyperparameters\n",
    "opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "latent_dim = 14\n",
    "n_epochs = 100\n",
    "n_batch = 32\n",
    "half_batch = int(n_batch / 2)\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator(latent_dim)\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "\n",
    "# train the model\n",
    "gan_trained = train(generator, discriminator, gan_model, X_train_adj, latent_dim, n_epochs, n_batch, half_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489c3d9-eccc-418a-8900-13204d4f29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate JS scores for GAN generated data and identify best GAN model iteration\n",
    "gan_model_js_scores = {}\n",
    "latent_points = generate_latent_points(latent_dim, len(X_train))\n",
    "\n",
    "def compile_gan_model_js_scores(n_steps, latent_points):\n",
    "    for i in range(n_epochs):\n",
    "        if (i+1) % 5 == 0: \n",
    "           \n",
    "            # generate synthetic data from generator\n",
    "            generator_saved_file = 'generator_model_%03d.h5' % (i+1)\n",
    "            generator_model_trained = load_model(generator_saved_file)\n",
    "            _X_synthetic_gan = generator_model_trained.predict(latent_points)\n",
    "            _X_synthetic_gan = scaler.inverse_transform(_X_synthetic_gan)\n",
    "            _X_synthetic_gan = np.round(_X_synthetic_gan)\n",
    "                        \n",
    "            # calculate JS scores and append\n",
    "            synthetic_train_JS_score_gan = scipy.spatial.distance.jensenshannon(_X_synthetic_gan, X_train, base=2)\n",
    "            gan_model_js_scores[i] = round(gmean(synthetic_train_JS_score_gan),4)\n",
    "\n",
    "compile_gan_model_js_scores(n_steps, latent_points)\n",
    "\n",
    "# return best model iteration\n",
    "min_value = min(gan_model_js_scores.values())\n",
    "min_key = min(gan_model_js_scores, key=gan_model_js_scores.get)+1\n",
    "print(f\"Best Model Iteration: {min_key}, Avg JS Score: {min_value}\")\n",
    "\n",
    "# return best 10 model iterartions\n",
    "def top_10_gan_models():\n",
    "    print(\"Top 10 GAN Model Iterations\")\n",
    "    top_10_gan_models = sorted(gan_model_js_scores, key=gan_model_js_scores.get, reverse=False)[:10]\n",
    "    for i in range(10):\n",
    "        key = top_10_gan_models[i]\n",
    "        value = gan_model_js_scores[key]\n",
    "        print(f\"Model Iteration: {key+1}, Avg JS Score: {value}\")\n",
    "\n",
    "top_10_gan_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb1342-b309-4410-9fec-880e6a651e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select GAN model\n",
    "gan_model_number = min_key\n",
    "\n",
    "# generate synthetic data using GAN\n",
    "generator_model_trained = load_model('generator_model_%03d.h5' % gan_model_number)\n",
    "latent_points = generate_latent_points(latent_dim, len(X_train))\n",
    "X_synthetic = generator_model_trained.predict(latent_points)\n",
    "\n",
    "# generate synthetic labels using GAN\n",
    "discriminator_model_trained = load_model('discriminator_model_%03d.h5' % gan_model_number)\n",
    "y_synthetic = discriminator_model_trained.predict(X_synthetic)\n",
    "\n",
    "# clean synthetic data\n",
    "X_synthetic = scaler.inverse_transform(X_synthetic)\n",
    "X_synthetic = np.round(X_synthetic)\n",
    "y_synthetic[y_synthetic<=0.5] = 0\n",
    "y_synthetic[y_synthetic>0.5] = 1\n",
    "print(X_synthetic[0:3])\n",
    "print(y_synthetic[0:3])\n",
    "\n",
    "# combine data into expanded dataset\n",
    "X_combined = np.concatenate((X_train, X_synthetic), axis=0)\n",
    "y_combined = np.concatenate((y_train, y_synthetic), axis=0)\n",
    "\n",
    "# calculate JS divergence of synthetic data relative to real data\n",
    "synthetic_train_JS_score = scipy.spatial.distance.jensenshannon(X_synthetic, X_train, base=2)\n",
    "print('Train JS Score: ' + str(np.round(synthetic_train_JS_score,4)))\n",
    "print('Avg JS Score: ' + str(round(gmean(synthetic_train_JS_score),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286582e-7fa7-4b61-9e04-6eb61df81c49",
   "metadata": {},
   "source": [
    "### Neural Network with GAN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df85bf5-0488-4aa0-a9bc-c75de99475ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data = 100% of real data\n",
    "\n",
    "# build model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(14, activation='LeakyReLU', input_dim=14))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "\n",
    "# set parameters\n",
    "early_stopping_gan = EarlyStopping(monitor='val_loss', mode='min', patience=400, verbose=1)\n",
    "model_save_gan = ModelCheckpoint('nn_gan_best_model.h5', save_best_only=True, monitor='val_auc', mode='max', verbose=1)\n",
    "\n",
    "#train GAN neural network model             \n",
    "nn_gan_model = nn_model.fit(X_combined, y_combined, validation_data=(X_test, y_test), epochs=2000, batch_size=32, verbose=1, callbacks=[early_stopping_gan, model_save_gan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4aa5c-9d82-47d6-a7b9-3bf55f3cb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "pyplot.plot(nn_gan_model.history['auc'], label='train')\n",
    "pyplot.plot(nn_gan_model.history['val_auc'], label='test')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92527157-f4fd-46f3-b733-a60262503351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load neural network model\n",
    "nn_gan_model_loaded = load_model('nn_gan_best_model.h5')\n",
    "\n",
    "# evaluate model on test data (how model would be evaluated at the time)\n",
    "predictions = nn_gan_model_loaded.predict(X_test)\n",
    "nn_gan_model_auc_test = roc_auc_score(y_test, predictions)\n",
    "print(\"Neural Net GAN Model AUC on Test Data: \" + str(round(nn_gan_model_auc_test,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d60e9-8336-42fa-9b2f-c3f2c8c87063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data = 50% of real data\n",
    "\n",
    "# generate synthetic data using GAN\n",
    "gan_model_number = min_key\n",
    "generator_model_trained = load_model('generator_model_%03d.h5' % gan_model_number)\n",
    "latent_points = generate_latent_points(latent_dim, int(len(X_train)/2))\n",
    "X_synthetic_half = generator_model_trained.predict(latent_points)\n",
    "\n",
    "# generate synthetic labels using GAN\n",
    "discriminator_model_trained = load_model('discriminator_model_%03d.h5' % gan_model_number)\n",
    "y_synthetic_half = discriminator_model_trained.predict(X_synthetic_half)\n",
    "\n",
    "# clean synthetic data\n",
    "X_synthetic_half = scaler.inverse_transform(X_synthetic_half)\n",
    "X_synthetic_half = np.round(X_synthetic_half)\n",
    "y_synthetic_half[y_synthetic_half<=0.5] = 0\n",
    "y_synthetic_half[y_synthetic_half>0.5] = 1\n",
    "\n",
    "# combine data into expanded dataset\n",
    "X_combined_half = np.concatenate((X_train, X_synthetic_half), axis=0)\n",
    "y_combined_half = np.concatenate((y_train, y_synthetic_half), axis=0)\n",
    "\n",
    "print(X_synthetic_half.shape)\n",
    "print(y_synthetic_half.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136b423-6a2e-46b7-95e1-c554324aa086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data = 50% of real data\n",
    "\n",
    "# build model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(14, activation='LeakyReLU', input_dim=14))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "\n",
    "# set parameters\n",
    "early_stopping_gan = EarlyStopping(monitor='val_loss', mode='min', patience=400, verbose=1)\n",
    "model_save_gan_half = ModelCheckpoint('nn_gan_half_best_model.h5', save_best_only=True, monitor='val_auc', mode='max', verbose=1)\n",
    "\n",
    "#train GAN neural network model             \n",
    "nn_gan_model_half = nn_model.fit(X_combined_half, y_combined_half, validation_data=(X_test, y_test), epochs=2000, batch_size=32, verbose=1, callbacks=[early_stopping_gan, model_save_gan_half])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81daa2e-be46-46f0-aa74-adcd229f1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "pyplot.plot(nn_gan_model_half.history['auc'], label='train')\n",
    "pyplot.plot(nn_gan_model_half.history['val_auc'], label='test')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c01b0-d2d0-492d-bfe5-3fc00e236ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load neural network model\n",
    "nn_gan_model_half_loaded = load_model('nn_gan_half_best_model.h5')\n",
    "\n",
    "# evaluate model on test data (how model would be evaluated at the time)\n",
    "predictions = nn_gan_model_half_loaded.predict(X_test)\n",
    "nn_gan_model_half_auc_test = roc_auc_score(y_test, predictions)\n",
    "print(\"Neural Net GAN Model 50% Sample AUC on Test Data: \" + str(round(nn_gan_model_half_auc_test,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce671aa-f855-4d47-b147-90299ca298b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data = 2x of real data\n",
    "\n",
    "# generate synthetic data using GAN\n",
    "gan_model_number = min_key\n",
    "generator_model_trained = load_model('generator_model_%03d.h5' % gan_model_number)\n",
    "latent_points = generate_latent_points(latent_dim, len(X_train)*2)\n",
    "X_synthetic_2x = generator_model_trained.predict(latent_points)\n",
    "\n",
    "# generate synthetic labels using GAN\n",
    "discriminator_model_trained = load_model('discriminator_model_%03d.h5' % gan_model_number)\n",
    "y_synthetic_2x = discriminator_model_trained.predict(X_synthetic_2x)\n",
    "\n",
    "# clean synthetic data\n",
    "X_synthetic_2x = scaler.inverse_transform(X_synthetic_2x)\n",
    "X_synthetic_2x = np.round(X_synthetic_2x)\n",
    "y_synthetic_2x[y_synthetic_2x<=0.5] = 0\n",
    "y_synthetic_2x[y_synthetic_2x>0.5] = 1\n",
    "\n",
    "# combine data into expanded dataset\n",
    "X_combined_2x = np.concatenate((X_train, X_synthetic_2x), axis=0)\n",
    "y_combined_2x = np.concatenate((y_train, y_synthetic_2x), axis=0)\n",
    "\n",
    "print(X_synthetic_2x.shape)\n",
    "print(y_synthetic_2x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f8292-6cf1-4701-a090-4bff856e0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data = 2x of real data\n",
    "\n",
    "# build model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(14, activation='LeakyReLU', input_dim=14))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "\n",
    "# set parameters\n",
    "early_stopping_gan = EarlyStopping(monitor='val_loss', mode='min', patience=400, verbose=1)\n",
    "model_save_gan_2x = ModelCheckpoint('nn_gan_2x_best_model.h5', save_best_only=True, monitor='val_auc', mode='max', verbose=1)\n",
    "\n",
    "#train GAN neural network model             \n",
    "nn_gan_model_2x = nn_model.fit(X_combined_2x, y_combined_2x, validation_data=(X_test, y_test), epochs=2000, batch_size=32, verbose=1, callbacks=[early_stopping_gan, model_save_gan_2x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a090d7-73f9-4dfa-b4ff-e14b75a20c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "pyplot.plot(nn_gan_model_2x.history['auc'], label='train')\n",
    "pyplot.plot(nn_gan_model_2x.history['val_auc'], label='test')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea56fac2-a286-40e8-9345-9b3e39832600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load neural network model\n",
    "nn_gan_model_2x_loaded = load_model('nn_gan_2x_best_model.h5')\n",
    "\n",
    "# evaluate model on test data (how model would be evaluated at the time)\n",
    "predictions = nn_gan_model_2x_loaded.predict(X_test)\n",
    "nn_gan_model_2x_auc_test = roc_auc_score(y_test, predictions)\n",
    "print(\"Neural Net GAN Model 2x Sample AUC on Test Data: \" + str(round(nn_gan_model_2x_auc_test,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaecb87-44a3-4a6f-9e0e-595436d5e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data = 5x of real data\n",
    "\n",
    "# generate synthetic data using GAN\n",
    "gan_model_number = min_key\n",
    "generator_model_trained = load_model('generator_model_%03d.h5' % gan_model_number)\n",
    "latent_points = generate_latent_points(latent_dim, len(X_train)*5)\n",
    "X_synthetic_5x = generator_model_trained.predict(latent_points)\n",
    "\n",
    "# generate synthetic labels using GAN\n",
    "discriminator_model_trained = load_model('discriminator_model_%03d.h5' % gan_model_number)\n",
    "y_synthetic_5x = discriminator_model_trained.predict(X_synthetic_5x)\n",
    "\n",
    "# clean synthetic data\n",
    "X_synthetic_5x = scaler.inverse_transform(X_synthetic_5x)\n",
    "X_synthetic_5x = np.round(X_synthetic_5x)\n",
    "y_synthetic_5x[y_synthetic_5x<=0.5] = 0\n",
    "y_synthetic_5x[y_synthetic_5x>0.5] = 1\n",
    "\n",
    "# combine data into expanded dataset\n",
    "X_combined_5x = np.concatenate((X_train, X_synthetic_5x), axis=0)\n",
    "y_combined_5x = np.concatenate((y_train, y_synthetic_5x), axis=0)\n",
    "\n",
    "print(X_synthetic_5x.shape)\n",
    "print(y_synthetic_5x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47495006-2066-4fa8-80d0-802fa64e7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data = 5x of real data\n",
    "\n",
    "# build model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(14, activation='LeakyReLU', input_dim=14))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "\n",
    "# set parameters\n",
    "early_stopping_gan = EarlyStopping(monitor='val_loss', mode='min', patience=400, verbose=1)\n",
    "model_save_gan_5x = ModelCheckpoint('nn_gan_5x_best_model.h5', save_best_only=True, monitor='val_auc', mode='max', verbose=1)\n",
    "\n",
    "#train GAN neural network model             \n",
    "nn_gan_model_5x = nn_model.fit(X_combined_5x, y_combined_5x, validation_data=(X_test, y_test), epochs=2000, batch_size=32, verbose=1, callbacks=[early_stopping_gan, model_save_gan_5x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda119a7-7df9-4bc9-a9a6-be97a51ff8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "pyplot.plot(nn_gan_model_5x.history['auc'], label='train')\n",
    "pyplot.plot(nn_gan_model_5x.history['val_auc'], label='test')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3d7af-1998-47b5-afbb-683448ca1ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load neural network model\n",
    "nn_gan_model_5x_loaded = load_model('nn_gan_5x_best_model.h5')\n",
    "\n",
    "# evaluate model on test data (how model would be evaluated at the time)\n",
    "predictions = nn_gan_model_5x_loaded.predict(X_test)\n",
    "nn_gan_model_5x_auc_test = roc_auc_score(y_test, predictions)\n",
    "print(\"Neural Net GAN Model 5x Sample AUC on Test Data: \" + str(round(nn_gan_model_5x_auc_test,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfa9c4-8721-4eee-8912-a066b0fa40bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select neural network model using GAN synthetic data with best test data AUC \n",
    "best_gan_version = nn_gan_model_half_loaded\n",
    "\n",
    "# evaluate best GAN synthetic data model on out of sample data (simulating a production environment)\n",
    "predictions = best_gan_version.predict(X_oos)\n",
    "nn_gan_model_auc_oos = roc_auc_score(y_oos, predictions)\n",
    "print(\"Neural Net GAN Model AUC on Out of Sample Data: \" + str(round(nn_gan_model_auc_oos,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caee10f-6fb9-4b36-8478-1633d8f583a6",
   "metadata": {},
   "source": [
    "## Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d41b0-1769-4f4e-810a-f750ac885d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize training data to 0 mean and unit variance\n",
    "scaler_2 = StandardScaler()\n",
    "X_train_adj_2 = scaler_2.fit_transform(X_train)  \n",
    "\n",
    "# define utility functions\n",
    "def make_beta_schedule(n_timesteps, schedule, start, end):\n",
    "    if schedule == 'linear':\n",
    "        betas = torch.linspace(start, end, n_timesteps)\n",
    "    elif schedule == \"quad\":\n",
    "        betas = torch.linspace(start ** 0.5, end ** 0.5, n_timesteps) ** 2\n",
    "    elif schedule == \"sigmoid\":\n",
    "        betas = torch.linspace(-6, 6, n_timesteps)\n",
    "        betas = torch.sigmoid(betas) * (end - start) + start\n",
    "    return betas\n",
    "\n",
    "def extract(input, t, x):\n",
    "    shape = x.shape\n",
    "    out = torch.gather(input, 0, t.to(input.device))\n",
    "    reshape = [t.shape[0]] + [1] * (len(shape) - 1)\n",
    "    return out.reshape(*reshape)\n",
    "\n",
    "# generate synthetic data\n",
    "def p_sample(model, x, t, alphas, betas, one_minus_alphas_bar_sqrt):\n",
    "    t = torch.tensor([t])\n",
    "    # Factor to the model output\n",
    "    eps_factor = ((1 - extract(alphas, t, x)) / extract(one_minus_alphas_bar_sqrt, t, x))\n",
    "    # Model output\n",
    "    eps_theta = model(x, t)\n",
    "    # Final values\n",
    "    mean = (1 / extract(alphas, t, x).sqrt()) * (x - (eps_factor * eps_theta))\n",
    "    # Generate z\n",
    "    z = torch.randn_like(x)\n",
    "    # Fixed sigma\n",
    "    sigma_t = extract(betas, t, x).sqrt()\n",
    "    sample = mean + sigma_t * z\n",
    "    return (sample)\n",
    "\n",
    "def p_sample_loop(model, shape, n_steps, alphas, betas, one_minus_alphas_bar_sqrt):\n",
    "    cur_x = torch.randn(shape)\n",
    "    x_seq = [cur_x]\n",
    "    for i in reversed(range(n_steps)):\n",
    "        cur_x = p_sample(model, cur_x, i, alphas, betas, one_minus_alphas_bar_sqrt)\n",
    "        x_seq.append(cur_x)\n",
    "    return x_seq\n",
    "\n",
    "# define loss function\n",
    "def noise_estimation_loss(model, x_0, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, n_steps):\n",
    "    batch_size = x_0.shape[0]\n",
    "    # Select a random step for each example\n",
    "    t = torch.randint(0, n_steps, size=(batch_size // 2 + 1,))\n",
    "    t = torch.cat([t, n_steps - t - 1], dim=0)[:batch_size].long()\n",
    "    # x0 multiplier\n",
    "    a = extract(alphas_bar_sqrt, t, x_0)\n",
    "    # eps multiplier\n",
    "    am1 = extract(one_minus_alphas_bar_sqrt, t, x_0)\n",
    "    e = torch.randn_like(x_0)\n",
    "    # model input\n",
    "    x = x_0 * a + e * am1\n",
    "    output = model(x, t)\n",
    "    return (e - output).square().mean()\n",
    "\n",
    "# core diffusion model architecture\n",
    "class ConditionalLinear(nn.Module):\n",
    "    def __init__(self, num_in, num_out, n_steps):\n",
    "        super(ConditionalLinear, self).__init__()\n",
    "        self.num_out = num_out\n",
    "        self.lin = nn.Linear(num_in, num_out)\n",
    "        self.embed = nn.Embedding(n_steps, num_out)\n",
    "        self.embed.weight.data.uniform_()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.lin(x)\n",
    "        gamma = self.embed(y)\n",
    "        out = gamma.view(-1, self.num_out) * out\n",
    "        return out\n",
    "        \n",
    "class ConditionalModel(nn.Module):\n",
    "    def __init__(self, n_steps):\n",
    "        super(ConditionalModel, self).__init__()\n",
    "        self.lin1 = ConditionalLinear(14, 56, n_steps)\n",
    "        self.norm1 = nn.GroupNorm(4, 56)\n",
    "        self.lin2 = ConditionalLinear(56, 44, n_steps)\n",
    "        self.norm2 = nn.GroupNorm(4, 44)\n",
    "        self.lin3 = ConditionalLinear(44, 28, n_steps)\n",
    "        self.norm3 = nn.GroupNorm(4, 28)\n",
    "        self.lin4 = nn.Linear(28, 14)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = F.softplus(self.lin1(x, y))\n",
    "        x = self.norm1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.softplus(self.lin2(x, y))\n",
    "        x = self.norm2(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.softplus(self.lin3(x, y))\n",
    "        x = self.norm3(x)\n",
    "        x = self.drop(x)\n",
    "        return self.lin4(x)\n",
    "\n",
    "# exponential moving average\n",
    "class EMA(object):\n",
    "    def __init__(self, mu):\n",
    "        self.mu = mu\n",
    "        self.shadow = {}\n",
    "\n",
    "    def register(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name].data = (1. - self.mu) * param.data + self.mu * self.shadow[name].data\n",
    "\n",
    "    def ema(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data.copy_(self.shadow[name].data)\n",
    "\n",
    "    def ema_copy(self, module):\n",
    "        module_copy = type(module)(module.config).to(module.config.device)\n",
    "        module_copy.load_state_dict(module.state_dict())\n",
    "        self.ema(module_copy)\n",
    "        return module_copy\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.shadow\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.shadow = state_dict\n",
    "        \n",
    "# set hyperparameters        \n",
    "epochs = 2001 \n",
    "n_steps = 100 \n",
    "batch_size = 64\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "betas = make_beta_schedule(n_timesteps=n_steps, schedule='linear', start=beta_start, end=beta_end)\n",
    "\n",
    "alphas = 1 - betas\n",
    "alphas_prod = torch.cumprod(alphas, 0)\n",
    "alphas_prod_p = torch.cat([torch.tensor([1]).float(), alphas_prod[:-1]], 0)\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)\n",
    "\n",
    "model = ConditionalModel(n_steps)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "data = X_train_adj_2.tolist()\n",
    "dataset = torch.tensor(data).float()\n",
    "table_dim = 14\n",
    "\n",
    "# create EMA model\n",
    "ema = EMA(0.999)\n",
    "ema.register(model)\n",
    "\n",
    "# run model\n",
    "for t in range(epochs):\n",
    "    permutation = torch.randperm(dataset.size()[0])\n",
    "    for i in range(0, dataset.size()[0], batch_size):\n",
    "        # retrieve current batch\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x = dataset[indices]\n",
    "        # calculate loss and train model\n",
    "        loss = noise_estimation_loss(model, batch_x, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, n_steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        optimizer.step()\n",
    "        # update exponential moving average\n",
    "        ema.update(model)\n",
    "    # print loss, generate synthetic data, and save outputs\n",
    "    if (t % 100 == 0):\n",
    "        print(f\"{t+1}/{epochs} Loss: {loss}\")\n",
    "        # to change the amount of synthetic data generated, adjust shape parameter in p_sample_loop\n",
    "        x_seq = p_sample_loop(model, dataset.shape, n_steps, alphas, betas, one_minus_alphas_bar_sqrt)\n",
    "        csv_path = 'diffusion_sythetic_data_%d.csv' % (t+1)        \n",
    "        pd.DataFrame(x_seq[n_steps].detach().numpy()).to_csv(csv_path, header=False, index=False)\n",
    "        torch.save({'epoch': t+1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss}, \n",
    "                    'diffusion_model_%03d.pth' % (t+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab898785-4e88-4d5a-99fc-19250031c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate JS scores for diffusion generated data and identify best diffusion model iteration\n",
    "diffusion_model_js_scores = {}\n",
    "\n",
    "def compile_diffusion_model_js_scores(epochs):\n",
    "    for i in range(epochs):\n",
    "        if i % 100 == 0 and i > 0:\n",
    "#            _X_synthetic_diffusion = eval('x_seq_%d' % (i+1))[n_steps]\n",
    "#            _X_synthetic_diffusion = _X_synthetic_diffusion.detach().numpy()\n",
    "            _X_synthetic_diffusion = pd.read_csv('diffusion_sythetic_data_%d.csv' % (i+1),header=None)\n",
    "            _X_synthetic_diffusion = scaler_2.inverse_transform(_X_synthetic_diffusion)\n",
    "            _X_synthetic_diffusion = tf.clip_by_value(_X_synthetic_diffusion, 0, X_train.max())\n",
    "            _X_synthetic_diffusion = np.round(_X_synthetic_diffusion)\n",
    "\n",
    "            # calculate JS scores and append\n",
    "            synthetic_train_JS_score_diffusion = scipy.spatial.distance.jensenshannon(_X_synthetic_diffusion, X_train, base=2)\n",
    "            diffusion_model_js_scores[i] = round(gmean(synthetic_train_JS_score_diffusion),4)\n",
    "\n",
    "compile_diffusion_model_js_scores(epochs)\n",
    "\n",
    "# return best model iteration\n",
    "min_value = min(diffusion_model_js_scores.values())\n",
    "min_key = min(diffusion_model_js_scores, key=diffusion_model_js_scores.get)\n",
    "print(f\"Best Model Iteration: {min_key}, Avg JS Score: {min_value}\")\n",
    "\n",
    "# return best 10 model iterartions\n",
    "def top_10_diffusion_models():\n",
    "    print(\"Top 10 Diffusion Model Iterations\")\n",
    "    top_10_diffusion_models = sorted(diffusion_model_js_scores, key=diffusion_model_js_scores.get, reverse=False)[:10]\n",
    "    for i in range(10):\n",
    "        key = top_10_diffusion_models[i]\n",
    "        value = diffusion_model_js_scores[key]\n",
    "        print(f\"Model Iteration: {key}, Avg JS Score: {value}\")\n",
    "\n",
    "top_10_diffusion_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae49f3-7fdf-4f41-a800-332a85223e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select diffusion model\n",
    "diffusion_model_number = min_key\n",
    "\n",
    "# generate synthetic data using diffusion\n",
    "X_synthetic_diffusion = pd.read_csv('diffusion_sythetic_data_%d.csv' % (min_key+1),header=None)\n",
    "X_synthetic_diffusion = scaler_2.inverse_transform(X_synthetic_diffusion)\n",
    "X_synthetic_diffusion = tf.clip_by_value(X_synthetic_diffusion, 0, X_train.max())\n",
    "X_synthetic_diffusion = np.round(X_synthetic_diffusion)\n",
    "\n",
    "\n",
    "# generate synthetic labels using neural network with no synthetic data\n",
    "y_synthetic_diffusion = nn_raw_model.predict(X_synthetic_diffusion)\n",
    "y_synthetic_diffusion[y_synthetic_diffusion<=0.5] = 0\n",
    "y_synthetic_diffusion[y_synthetic_diffusion>0.5] = 1\n",
    "\n",
    "print(X_synthetic_diffusion[0:3])\n",
    "print(y_synthetic_diffusion[0:3])\n",
    "\n",
    "# combine data into expanded dataset\n",
    "X_combined_diffusion = np.concatenate((X_train, X_synthetic_diffusion), axis=0)\n",
    "y_combined_diffusion = np.concatenate((y_train, y_synthetic_diffusion), axis=0)\n",
    "\n",
    "# calculate JS divergence of synthetic data relative to real data\n",
    "synthetic_train_JS_score = scipy.spatial.distance.jensenshannon(X_synthetic_diffusion, X_train, base=2)\n",
    "print('Train JS Score: ' + str(np.round(synthetic_train_JS_score,4)))\n",
    "print('Avg JS Score: ' + str(round(gmean(synthetic_train_JS_score),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d0792-91a1-4a08-b035-30b24129025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate alternative sized synthetic data sample\n",
    "\n",
    "# set data scalar\n",
    "data_scalar = 0.5\n",
    "data_size = [round(data_scalar * len(X_train)), len(X_train.columns)]\n",
    "\n",
    "# load diffusion model\n",
    "model = ConditionalModel(n_steps)\n",
    "checkpoint = torch.load('diffusion_model_%03d.pth' % (diffusion_model_number+1))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# generate synthetic data\n",
    "x_seq_scaled = p_sample_loop(model, torch.ones(data_size).shape, n_steps, alphas, betas, one_minus_alphas_bar_sqrt)\n",
    "X_synthetic_diffusion_scaled = x_seq_scaled[n_steps]\n",
    "X_synthetic_diffusion_scaled = X_synthetic_diffusion_scaled.detach().numpy()\n",
    "X_synthetic_diffusion_scaled = scaler_2.inverse_transform(X_synthetic_diffusion_scaled)\n",
    "X_synthetic_diffusion_scaled = tf.clip_by_value(X_synthetic_diffusion_scaled, 0, X_train.max())\n",
    "X_synthetic_diffusion_scaled = np.round(X_synthetic_diffusion_scaled)\n",
    "\n",
    "# generate synthetic labels using neural network with no synthetic data\n",
    "y_synthetic_diffusion_scaled = nn_raw_model.predict(X_synthetic_diffusion_scaled)\n",
    "y_synthetic_diffusion_scaled[y_synthetic_diffusion_scaled<=0.5] = 0\n",
    "y_synthetic_diffusion_scaled[y_synthetic_diffusion_scaled>0.5] = 1\n",
    "\n",
    "# combine data into expanded dataset\n",
    "X_combined_diffusion_scaled = np.concatenate((X_train, X_synthetic_diffusion_scaled), axis=0)\n",
    "y_combined_diffusion_scaled = np.concatenate((y_train, y_synthetic_diffusion_scaled), axis=0)\n",
    "\n",
    "print(X_combined_diffusion_scaled.shape)\n",
    "print(y_combined_diffusion_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e0378-ed79-4912-a14d-5f2ba9cacd95",
   "metadata": {},
   "source": [
    "### Neural Network with Diffusion Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935bc21c-2b55-4b33-b0f4-924a8a294a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data = 100% of real data\n",
    "\n",
    "# build model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(14, activation='LeakyReLU', input_dim=14))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "\n",
    "# set parameters\n",
    "early_stopping_diffusion = EarlyStopping(monitor='val_loss', mode='min', patience=400, verbose=1)\n",
    "model_save_diffusion = ModelCheckpoint('nn_diffusion_best_model.h5', save_best_only=True, monitor='val_auc', mode='max', verbose=1)\n",
    "\n",
    "#train diffusion neural network model             \n",
    "nn_diffusion_model = nn_model.fit(X_combined_diffusion, y_combined_diffusion, validation_data=(X_test, y_test), epochs=2000, batch_size=32, verbose=1, callbacks=[early_stopping_diffusion, model_save_diffusion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24dfb9-afca-4551-b35e-7c8666f5df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "pyplot.plot(nn_diffusion_model.history['auc'], label='train')\n",
    "pyplot.plot(nn_diffusion_model.history['val_auc'], label='test')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e8e69-71f5-4867-9d8f-190b7f2a1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load neural network model\n",
    "nn_diffusion_model_loaded = load_model('nn_diffusion_best_model.h5')\n",
    "\n",
    "# evaluate model on test data (how model would be evaluated at the time)\n",
    "predictions = nn_diffusion_model_loaded.predict(X_test)\n",
    "nn_diffusion_model_auc_test = roc_auc_score(y_test, predictions)\n",
    "print(\"Neural Net Diffusion Model AUC on Test Data: \" + str(round(nn_diffusion_model_auc_test,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9dd937-e3cc-4596-824c-c53d98f913f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data = 50% of real data\n",
    "\n",
    "# build model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(14, activation='LeakyReLU', input_dim=14))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "\n",
    "# set parameters\n",
    "early_stopping_diffusion = EarlyStopping(monitor='val_loss', mode='min', patience=400, verbose=1)\n",
    "model_save_diffusion_half = ModelCheckpoint('nn_diffusion_best_model_half.h5', save_best_only=True, monitor='val_auc', mode='max', verbose=1)\n",
    "\n",
    "#train GAN neural network model             \n",
    "nn_diffusion_model_half = nn_model.fit(X_combined_diffusion_scaled, y_combined_diffusion_scaled, validation_data=(X_test, y_test), epochs=2000, batch_size=32, verbose=1, callbacks=[early_stopping_diffusion, model_save_diffusion_half])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4741888-1b2d-4f7a-8b94-4b51c35bdb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "pyplot.plot(nn_diffusion_model_half.history['auc'], label='train')\n",
    "pyplot.plot(nn_diffusion_model_half.history['val_auc'], label='test')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3613181-6b41-40ee-9c0c-601261d46a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load neural network model\n",
    "nn_diffusion_model_half_loaded = load_model('nn_diffusion_best_model_half.h5')\n",
    "\n",
    "# evaluate model on test data (how model would be evaluated at the time)\n",
    "predictions = nn_diffusion_model_half_loaded.predict(X_test)\n",
    "nn_diffusion_model_auc_test = roc_auc_score(y_test, predictions)\n",
    "print(\"Neural Net Diffusion Model 50% Sample AUC on Test Data: \" + str(round(nn_diffusion_model_auc_test,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e37ac0-dddc-43a6-b64f-8a7513448db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data = 2x of real data\n",
    "\n",
    "# build model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(14, activation='LeakyReLU', input_dim=14))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(32, activation='LeakyReLU'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "\n",
    "# set parameters\n",
    "early_stopping_diffusion = EarlyStopping(monitor='val_loss', mode='min', patience=400, verbose=1)\n",
    "model_save_diffusion_2x = ModelCheckpoint('nn_diffusion_best_model_2x.h5', save_best_only=True, monitor='val_auc', mode='max', verbose=1)\n",
    "\n",
    "#train GAN neural network model             \n",
    "nn_diffusion_model_2x = nn_model.fit(X_combined_diffusion_scaled, y_combined_diffusion_scaled, validation_data=(X_test, y_test), epochs=2000, batch_size=32, verbose=1, callbacks=[early_stopping_diffusion, model_save_diffusion_2x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4cace-aea1-4bd6-8dd6-5d855c2dc0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "pyplot.plot(nn_diffusion_model_2x.history['auc'], label='train')\n",
    "pyplot.plot(nn_diffusion_model_2x.history['val_auc'], label='test')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeda611-997f-4fd0-ad51-4226f5854550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load neural network model\n",
    "nn_diffusion_model_2x_loaded = load_model('nn_diffusion_best_model_2x.h5')\n",
    "\n",
    "# evaluate model on test data (how model would be evaluated at the time)\n",
    "predictions = nn_diffusion_model_2x_loaded.predict(X_test)\n",
    "nn_diffusion_model_auc_test = roc_auc_score(y_test, predictions)\n",
    "print(\"Neural Net Diffusion Model 2x Sample AUC on Test Data: \" + str(round(nn_diffusion_model_auc_test,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d7ffd-0fd5-4939-9e04-9ca563abcafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select neural network model using diffusion mdel synthetic data with best test data AUC \n",
    "best_diffusion_version = nn_diffusion_model_half_loaded\n",
    "\n",
    "# evaluate model on out of sample data (simulating a production environment)\n",
    "predictions = best_diffusion_version.predict(X_oos)\n",
    "nn_diffusion_model_auc_oos = roc_auc_score(y_oos, predictions)\n",
    "print(\"Neural Net Diffusion Model AUC on Out of Sample Data: \" + str(round(nn_diffusion_model_auc_oos,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ec549-f622-4ed6-a171-788926b5c347",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aaab1b-2500-42bb-a6d9-735870a96ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Neural Net with No Synthetic Data: {}\".format(round(nn_raw_model_auc_oos,4)))\n",
    "print(\"Neural Net with GAN Synthetic Data: {}\".format(round(nn_gan_model_auc_oos,4)))\n",
    "print(\"Neural Net with Diffusion Synthetic Data: {}\".format(round(nn_diffusion_model_auc_oos,4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
